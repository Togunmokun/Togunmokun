{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1FsQnUFsJYLASr7292DfXDAH4ICZipT8h",
      "authorship_tag": "ABX9TyPsiFOQD8Vb2XKawKlBd8lD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DSXpYFcZuRTt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "from tensorflow import keras\n",
        "from imblearn.over_sampling import SMOTE\n",
        "!pip install keras-self-attention\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
        "import os\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "import logging\n",
        "from keras.callbacks import Callback\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# List of parquet file paths\n",
        "parquet_files = [\n",
        "    '/content/drive/MyDrive/Parquetfile/Botnet-Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "    '/content/drive/MyDrive/Parquetfile/Bruteforce-Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "    '/content/drive/MyDrive/Parquetfile/DDoS1-Tuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "    '/content/drive/MyDrive/Parquetfile/DDoS2-Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "    '/content/drive/MyDrive/Parquetfile/DoS1-Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "    '/content/drive/MyDrive/Parquetfile/DoS2-Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "    '/content/drive/MyDrive/Parquetfile/Infil1-Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "    '/content/drive/MyDrive/Parquetfile/Infil2-Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "    '/content/drive/MyDrive/Parquetfile/Web1-Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "    '/content/drive/MyDrive/Parquetfile/Web2-Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet',\n",
        "]\n",
        "\n",
        "# Destination directory for saving CSV files\n",
        "csv_directory = '/content/drive/MyDrive/CSV_Data/'\n",
        "\n",
        "# Convert and save parquet files as CSV\n",
        "for parquet_file in parquet_files:\n",
        "    file_name = parquet_file.split('/')[-1].replace('.parquet', '.csv')\n",
        "    df = pd.read_parquet(parquet_file)\n",
        "    df.to_csv(csv_directory + file_name, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "LABELS_7 = {\n",
        "    'Benign': 0,\n",
        "    'SSH-Bruteforce': 1,\n",
        "    'FTP-BruteForce': 1,\n",
        "    'Brute Force -Web': 2,\n",
        "    'Brute Force -XSS': 2,\n",
        "    'SQL Injection': 2,\n",
        "    'DDOS attack-HOIC': 3,\n",
        "    'DDOS attack-LOIC-UDP': 3,\n",
        "    'DDoS attacks-LOIC-HTTP': 3,\n",
        "    'DoS attacks-Slowloris': 4,\n",
        "    'DoS attacks-Hulk': 4,\n",
        "    'DoS attacks-GoldenEye': 4,\n",
        "    'DoS attacks-SlowHTTPTest': 4,\n",
        "    'Bot': 5,\n",
        "    'Infilteration': 6,\n",
        "}\n",
        "\n",
        "def preprocess_labels(df):\n",
        "    df['Label'] = df['Label'].replace([k for k, w in LABELS_7.items()], [w for k, w in LABELS_7.items()])\n",
        "\n",
        "def load_and_preprocess(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    preprocess_labels(df)\n",
        "    return df\n",
        "\n",
        "# List of CSV file paths\n",
        "csv_files = [\n",
        "    csv_directory + 'Botnet-Friday-02-03-2018_TrafficForML_CICFlowMeter.csv',\n",
        "    csv_directory + 'Bruteforce-Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv',\n",
        "    csv_directory + 'DDoS1-Tuesday-20-02-2018_TrafficForML_CICFlowMeter.csv',\n",
        "    csv_directory + 'DDoS2-Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv',\n",
        "    csv_directory + 'DoS1-Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv',\n",
        "    csv_directory + 'DoS2-Friday-16-02-2018_TrafficForML_CICFlowMeter.csv',\n",
        "    csv_directory + 'Infil1-Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv',\n",
        "    csv_directory + 'Infil2-Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv',\n",
        "    csv_directory + 'Web1-Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv',\n",
        "    csv_directory + 'Web2-Friday-23-02-2018_TrafficForML_CICFlowMeter.csv',\n",
        "]\n",
        "\n",
        "# Define the load_dataset function\n",
        "def load_dataset(filenames):\n",
        "    dataframes = [load_and_preprocess(csv_file) for csv_file in csv_files]\n",
        "    combined_df = pd.concat(dataframes, axis=0, ignore_index=True)\n",
        "    class_num = 7\n",
        "    train_labels = combined_df.pop(\"Label\")\n",
        "    train_x = combined_df.values\n",
        "    train_y = tf.keras.utils.to_categorical(train_labels, class_num)\n",
        "    return train_x, train_y\n"
      ],
      "metadata": {
        "id": "KL5Wh1LNALwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b345fd-2516-4823-8271-41d335168877"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.10/dist-packages (0.51.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming train_y is a numpy array with one-hot encoded labels\n",
        "# You can convert it back to a list of labels\n",
        "labels = np.argmax(train_y, axis=1)\n",
        "\n",
        "# Calculate the label distribution using Counter\n",
        "label_distribution = collections.Counter(labels)\n",
        "\n",
        "# Print the label distribution\n",
        "print(\"Label Distribution:\")\n",
        "for label, count in label_distribution.items():\n",
        "    print(f\"Label {label}: Count {count}\")"
      ],
      "metadata": {
        "id": "ssM9Wu-up2W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset using the load_dataset function\n",
        "train_x, train_y = load_dataset(csv_files)\n",
        "\n",
        "\n",
        "# Specify the classes you want to oversample (classes 1 to 6)\n",
        "classes_to_oversample = [1, 2, 4, 5, 6]\n",
        "\n",
        "# Initialize the SMOTE sampler with a sampling strategy of 300,000 for the selected classes\n",
        "oversample = SMOTE(sampling_strategy={cls: 300000 for cls in classes_to_oversample})\n",
        "\n",
        "# Perform oversampling on the selected classes\n",
        "train_x_resampled, train_y_resampled = oversample.fit_resample(train_x, train_y)\n"
      ],
      "metadata": {
        "id": "gVZUJ8_EBe-J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming train_y_resampled is a numpy array with one-hot encoded labels\n",
        "# You can convert it back to a list of labels\n",
        "labels = np.argmax(train_y_resampled, axis=1)\n",
        "\n",
        "# Calculate the label distribution using Counter\n",
        "label_distribution = collections.Counter(labels)\n",
        "\n",
        "# Print the label distribution\n",
        "print(\"Label Distribution:\")\n",
        "for label, count in label_distribution.items():\n",
        "    print(f\"Label {label}: Count {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8uYJxEfBlcm",
        "outputId": "84fc97cd-a65a-4ab4-acfe-6cf35d6af77d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Distribution:\n",
            "Label 0: Count 5329008\n",
            "Label 5: Count 300000\n",
            "Label 1: Count 300000\n",
            "Label 3: Count 775955\n",
            "Label 4: Count 300000\n",
            "Label 6: Count 300000\n",
            "Label 2: Count 300000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a logger\n",
        "log = logging.getLogger(__name__)\n",
        "log.setLevel(logging.DEBUG)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split your data into training (80%) and temporary (20%) sets\n",
        "temp_x, test_x, temp_y, test_y = train_test_split(train_x_resampled, train_y_resampled, test_size=0.2, random_state=42, shuffle=True, stratify=train_y_resampled)\n",
        "\n",
        "# Split the temporary set into training (80%) and validation (20%) sets\n",
        "train_x, val_x, train_y, val_y = train_test_split(temp_x, temp_y, test_size=0.2, random_state=42, shuffle=True, stratify=temp_y)\n",
        "\n",
        "# Now, train_x, train_y, val_x, val_y, test_x, and test_y have a stratified split of your data.\n",
        "\n",
        "\n",
        "# Debugging: Print shapes\n",
        "print(\"Train Data Shapes:\")\n",
        "print(\"train_x shape:\", train_x.shape)\n",
        "print(\"train_y shape:\", train_y.shape)\n",
        "\n",
        "print(\"Validation Data Shapes:\")\n",
        "print(\"val_x shape:\", val_x.shape)\n",
        "print(\"val_y shape:\", val_y.shape)\n",
        "\n",
        "print(\"Test Data Shapes:\")\n",
        "print(\"test_x shape:\", test_x.shape)\n",
        "print(\"test_y shape:\", test_y.shape)\n",
        "\n",
        "# Assuming train_y_resampled is a numpy array with one-hot encoded labels\n",
        "# You can convert it back to a list of labels\n",
        "labels = np.argmax(train_y_resampled, axis=1)\n",
        "\n",
        "# Calculate the label distribution using Counter\n",
        "label_distribution = collections.Counter(labels)\n",
        "\n",
        "# Print the label distribution\n",
        "print(\"Label Distribution:\")\n",
        "for label, count in label_distribution.items():\n",
        "    print(f\"Label {label}: Count {count}\")"
      ],
      "metadata": {
        "id": "F4ut7xVpiADQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e234deae-99dc-42d8-bc70-fdeedab05432"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shapes:\n",
            "train_x shape: (4867176, 77)\n",
            "train_y shape: (4867176, 7)\n",
            "Validation Data Shapes:\n",
            "val_x shape: (1216794, 77)\n",
            "val_y shape: (1216794, 7)\n",
            "Test Data Shapes:\n",
            "test_x shape: (1520993, 77)\n",
            "test_y shape: (1520993, 7)\n",
            "Label Distribution:\n",
            "Label 0: Count 5329008\n",
            "Label 5: Count 300000\n",
            "Label 1: Count 300000\n",
            "Label 3: Count 775955\n",
            "Label 4: Count 300000\n",
            "Label 6: Count 300000\n",
            "Label 2: Count 300000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the datasets\n",
        "train_x = train_x.reshape(-1, 77, 1)\n",
        "val_x = val_x.reshape(-1, 77, 1)\n",
        "test_x = test_x.reshape(-1, 77, 1)\n",
        "# Debugging: Print shapes\n",
        "print(\"Train Data Shapes:\")\n",
        "print(\"train_x shape:\", train_x.shape)\n",
        "print(\"train_y shape:\", train_y.shape)\n",
        "\n",
        "print(\"Validation Data Shapes:\")\n",
        "print(\"val_x shape:\", val_x.shape)\n",
        "print(\"val_y shape:\", val_y.shape)\n",
        "\n",
        "print(\"Test Data Shapes:\")\n",
        "print(\"test_x shape:\", test_x.shape)\n",
        "print(\"test_y shape:\", test_y.shape)\n",
        "\n",
        "# Assuming train_y_resampled is a numpy array with one-hot encoded labels\n",
        "# You can convert it back to a list of labels\n",
        "labels = np.argmax(train_y_resampled, axis=1)\n",
        "\n",
        "# Calculate the label distribution using Counter\n",
        "label_distribution = collections.Counter(labels)\n",
        "\n",
        "# Print the label distribution\n",
        "print(\"Label Distribution:\")\n",
        "for label, count in label_distribution.items():\n",
        "    print(f\"Label {label}: Count {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpBZ5n1jhaN5",
        "outputId": "42af7dfe-0f9a-4ddf-9f3f-74a047f5da7c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shapes:\n",
            "train_x shape: (4867176, 77, 1)\n",
            "train_y shape: (4867176, 7)\n",
            "Validation Data Shapes:\n",
            "val_x shape: (1216794, 77, 1)\n",
            "val_y shape: (1216794, 7)\n",
            "Test Data Shapes:\n",
            "test_x shape: (1520993, 77, 1)\n",
            "test_y shape: (1520993, 7)\n",
            "Label Distribution:\n",
            "Label 0: Count 5329008\n",
            "Label 5: Count 300000\n",
            "Label 1: Count 300000\n",
            "Label 3: Count 775955\n",
            "Label 4: Count 300000\n",
            "Label 6: Count 300000\n",
            "Label 2: Count 300000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check shapes and data types\n",
        "print(\"test_x shape:\", test_x.shape)\n",
        "print(\"test_y shape:\", test_y.shape)\n",
        "print(\"test_x data type:\", test_x.dtype)\n",
        "print(\"test_y data type:\", test_y.dtype)\n",
        "\n",
        "# Check for NaN and infinite values\n",
        "if np.isnan(test_x).any() or np.isinf(test_x).any():\n",
        "    print(\"test_x contains NaN or infinite values.\")\n",
        "else:\n",
        "    print(\"test_x does not contain NaN or infinite values.\")\n",
        "\n",
        "if np.isnan(test_y).any() or np.isinf(test_y).any():\n",
        "    print(\"test_y contains NaN or infinite values.\")\n",
        "else:\n",
        "    print(\"test_y does not contain NaN or infinite values.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4eSLixIix-P",
        "outputId": "ee6e6167-4b1b-4cef-beff-79302533621e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_x shape: (1520993, 77, 1)\n",
            "test_y shape: (1520993, 7)\n",
            "test_x data type: float64\n",
            "test_y data type: int64\n",
            "test_x does not contain NaN or infinite values.\n",
            "test_y does not contain NaN or infinite values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply data reduction techniques here\n",
        "sample_fraction = 0.7\n",
        "num_samples_train = int(len(train_x) * sample_fraction)\n",
        "train_x, train_y = train_x[:num_samples_train], train_y[:num_samples_train]\n",
        "# Debugging: Print shapes\n",
        "print(\"Train Data Shapes:\")\n",
        "print(\"train_x shape:\", train_x.shape)\n",
        "print(\"train_y shape:\", train_y.shape)\n",
        "\n",
        "print(\"val Data Shapes:\")\n",
        "print(\"val_x shape:\", val_x.shape)\n",
        "print(\"val_y shape:\", val_y.shape)\n",
        "\n",
        "print(\"Test Data Shapes:\")\n",
        "print(\"test_x shape:\", test_x.shape)\n",
        "print(\"test_y shape:\", test_y.shape)\n",
        "\n",
        "# Assuming train_y_resampled is a numpy array with one-hot encoded labels\n",
        "# You can convert it back to a list of labels\n",
        "labels = np.argmax(train_y_resampled, axis=1)\n",
        "\n",
        "# Calculate the label distribution using Counter\n",
        "label_distribution = collections.Counter(labels)\n",
        "\n",
        "# Print the label distribution\n",
        "print(\"Label Distribution:\")\n",
        "for label, count in label_distribution.items():\n",
        "    print(f\"Label {label}: Count {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRIKHjpFSiSe",
        "outputId": "337901a0-52ea-461a-a903-168e7a2d289d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shapes:\n",
            "train_x shape: (3407023, 77, 1)\n",
            "train_y shape: (3407023, 7)\n",
            "val Data Shapes:\n",
            "val_x shape: (1216794, 77, 1)\n",
            "val_y shape: (1216794, 7)\n",
            "Test Data Shapes:\n",
            "test_x shape: (1520993, 77, 1)\n",
            "test_y shape: (1520993, 7)\n",
            "Label Distribution:\n",
            "Label 0: Count 5329008\n",
            "Label 5: Count 300000\n",
            "Label 1: Count 300000\n",
            "Label 3: Count 775955\n",
            "Label 4: Count 300000\n",
            "Label 6: Count 300000\n",
            "Label 2: Count 300000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameters dictionary\n",
        "sampling_parameters = [\n",
        "    {2: 100000},\n",
        "    {2: 150000},\n",
        "    {2: 200000},\n",
        "    {2: 250000},\n",
        "]\n",
        "\n",
        "DNN_parameters = [\n",
        "    {'lstm1': 256, 'lstm2': 256, 'att': 256, 'lstm3': 128, 'dense1': 100, 'dense2': 80},\n",
        "    {'lstm1': 128, 'lstm2': 128, 'att': 128, 'lstm3': 92, 'dense1': 80, 'dense2': 80},\n",
        "    {'lstm1': 64, 'lstm2': 64, 'att': 64, 'lstm3': 64, 'dense1': 64, 'dense2': 32}\n",
        "]"
      ],
      "metadata": {
        "id": "CjAHRaIIsxrD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming train_y_resampled is a numpy array with one-hot encoded labels\n",
        "# You can convert it back to a list of labels\n",
        "labels = np.argmax(train_y, axis=1)\n",
        "\n",
        "# Calculate the label distribution using Counter\n",
        "label_distribution = collections.Counter(labels)\n",
        "\n",
        "# Print the label distribution\n",
        "print(\"Label Distribution:\")\n",
        "for label, count in label_distribution.items():\n",
        "    print(f\"Label {label}: Count {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCLN7HzXP_S1",
        "outputId": "43869e5e-0ebe-42e2-de76-8c043a9c0e1f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Distribution:\n",
            "Label 0: Count 2386814\n",
            "Label 2: Count 134697\n",
            "Label 3: Count 347521\n",
            "Label 1: Count 134676\n",
            "Label 4: Count 134460\n",
            "Label 6: Count 134519\n",
            "Label 5: Count 134336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the predictions dictionary\n",
        "predictions = {}\n",
        "models = {}  # Dictionary to store models\n",
        "\n",
        "for num1, samp_par in enumerate(sampling_parameters):\n",
        "    for num2, net_par in enumerate(DNN_parameters):\n",
        "        # Set up callbacks\n",
        "        log_dir = os.path.join('logs/long_fit_GRU/' + \"s%s_n%s/\" % (num1, num2))\n",
        "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=10000)\n",
        "\n",
        "        # Create and compile the model\n",
        "        model = Sequential([\n",
        "            LSTM(net_par['lstm1'], input_shape=(77, 1), return_sequences=True),\n",
        "            LSTM(net_par['lstm2'], return_sequences=True, dropout=0.1),\n",
        "            SeqSelfAttention(attention_width=net_par['att'], attention_activation='sigmoid', name='Attention'),\n",
        "            LSTM(net_par['lstm3'], dropout=0.1),\n",
        "            Dense(net_par['dense1'], activation='relu'),\n",
        "            Dense(net_par['dense2'], activation='relu'),\n",
        "            Dense(7, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        # Compile the model\n",
        "        sgdm_optimizer = SGD(learning_rate=0.1, momentum=0.9)\n",
        "        model.compile(optimizer=sgdm_optimizer, loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly=True)\n",
        "\n",
        "        # Store the model with a unique name\n",
        "        model_name = \"GRU_model_s%s_n%s\" % (num1, num2)\n",
        "        models[model_name] = model\n",
        "\n",
        "        # Now, you can use train_x, train_y, val_x, val_y, test_x, and test_y to fit and evaluate the model.\n",
        "    # Print shapes for debugging\n",
        "print(\"Train X Shape:\", train_x.shape)\n",
        "print(\"Train Y Shape:\", train_y.shape)\n",
        "print(\"Val X Shape:\", val_x.shape)\n",
        "print(\"Val Y Shape:\", val_y.shape)\n",
        "print(\"Test X Shape:\", test_x.shape)\n",
        "print(\"Test Y Shape:\", test_y.shape)\n",
        "\n",
        "# Assuming train_y_resampled is a numpy array with one-hot encoded labels\n",
        "# You can convert it back to a list of labels\n",
        "labels = np.argmax(train_y_resampled, axis=1)\n",
        "\n",
        "# Calculate the label distribution using Counter\n",
        "label_distribution = collections.Counter(labels)\n",
        "\n",
        "# Print the label distribution\n",
        "print(\"Label Distribution:\")\n",
        "for label, count in label_distribution.items():\n",
        "    print(f\"Label {label}: Count {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL1zMVC2F6t1",
        "outputId": "e62f6413-d222-4e1f-bea7-e63741adaaa5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X Shape: (3407023, 77, 1)\n",
            "Train Y Shape: (3407023, 7)\n",
            "Val X Shape: (1216794, 77, 1)\n",
            "Val Y Shape: (1216794, 7)\n",
            "Test X Shape: (1520993, 77, 1)\n",
            "Test Y Shape: (1520993, 7)\n",
            "Label Distribution:\n",
            "Label 0: Count 5329008\n",
            "Label 5: Count 300000\n",
            "Label 1: Count 300000\n",
            "Label 3: Count 775955\n",
            "Label 4: Count 300000\n",
            "Label 6: Count 300000\n",
            "Label 2: Count 300000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "            train_x,\n",
        "            train_y,\n",
        "            epochs=1,\n",
        "            batch_size=512,\n",
        "            validation_data=(val_x, val_y),\n",
        "            callbacks=[tensorboard_callback]\n",
        "        )\n",
        "\n",
        "# Save the trained model in the recommended Keras format\n",
        "save_dir = 'GRU_models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model.save(os.path.join(save_dir, model_name + '.keras'))\n",
        "\n"
      ],
      "metadata": {
        "id": "oIZN8qEfCYfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d624b3b-4f1e-4fbc-8aaa-c2d8fe242096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3320/6655 [=============>................] - ETA: 6:16 - loss: 0.5694 - accuracy: 0.8160"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming train_y_resampled is a numpy array with one-hot encoded labels\n",
        "# You can convert it back to a list of labels\n",
        "labels = np.argmax(test_y, axis=1)\n",
        "\n",
        "# Calculate the label distribution using Counter\n",
        "label_distribution = collections.Counter(labels)\n",
        "\n",
        "# Print the label distribution\n",
        "print(\"Label Distribution:\")\n",
        "for label, count in label_distribution.items():\n",
        "    print(f\"Label {label}: Count {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR9_Ov1nPhfM",
        "outputId": "514c9ba1-f30a-4e55-bdea-60148fe54df5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Distribution:\n",
            "Label 3: Count 155191\n",
            "Label 0: Count 1065802\n",
            "Label 5: Count 60000\n",
            "Label 6: Count 60000\n",
            "Label 1: Count 60000\n",
            "Label 2: Count 60000\n",
            "Label 4: Count 60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = load_model(os.path.join(save_dir, model_name + '.keras'), custom_objects={'SeqSelfAttention': SeqSelfAttention})\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "predictions[model_name] = model.predict(test_x)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('===============')\n",
        "print(\"Loss:\", history.history['loss'][0])\n",
        "print(\"Accuracy:\", history.history['accuracy'][0])\n",
        "print(confusion_matrix(test_y.argmax(axis=1), predictions[model_name].argmax(axis=1)))\n",
        "print(classification_report(test_y.argmax(axis=1), predictions[model_name].argmax(axis=1)))\n",
        "print(num1, num2)"
      ],
      "metadata": {
        "id": "D7wm6Rm01f5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert true labels and predictions to class indices\n",
        "true_labels = np.argmax(test_y, axis=1)\n",
        "predicted_labels = np.argmax(predictions[model_name], axis=1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(true_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "UvWJyoxDJ5Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsQxDbZAKC6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}